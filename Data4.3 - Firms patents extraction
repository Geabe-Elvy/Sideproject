import pandas as pd
import numpy as np
import requests
import json
import time as tm
import math
import networkx as nx
import matplotlib.pyplot as plt
from tkinter import filedialog
import pycountry
from datetime import datetime
from dateutil import parser

path = "C:/Users/vernhes/OneDrive - ENSTA Paris/Bureau/Sideproject/Data4/"
dfirm = pd.read_excel(f"{path}df_2000_Green_Clean_Tech_Compagnies.xlsx")
dfund_usd = pd.read_csv(f"{path}dfund_usd.csv")
df_pers = pd.read_excel(f"{path}df_pers.xlsx")

# Fonction pour extraire la dernière partie après la dernière virgule
def extract_last_part(location):
    parts = location.split(',')
    if parts:  # Vérifier si la liste n'est pas vide
        return parts[-1].strip()  # Enlever les espaces de début et de fin
    return location  # Retourner la chaîne originale si elle ne contient pas de virgule

# Appliquer la fonction à chaque ligne de la colonne 'location'
dfirm['Headquarters Location'] = dfirm['Headquarters Location'].astype(str)
dfirm['Jurisdiction'] = dfirm['Headquarters Location'].apply(extract_last_part)
for i in range(0, len(dfirm)):
    print(f"{i}")
    try:
        dfirm['Jurisdiction'][i] = pycountry.countries.search_fuzzy(dfirm['Jurisdiction'][i])[0].alpha_2
    except:
        i+=1

from dateutil import parser
import datetime
# Fonction pour convertir chaque entrée en année
def extract_year(date):
    if isinstance(date, int):  # Si c'est déjà un entier, c'est déjà une année
        return date
    elif isinstance(date, str):  # Si c'est une chaîne, analyser la date pour extraire l'année
        return parser.parse(date).year
    elif isinstance(date, datetime.datetime):  # Si c'est un objet datetime, obtenir l'année directement
        return date.year

# Appliquer la fonction à chaque élément de la liste et éliminer les doublons
dfirm['Founding Year'] = company_date = [extract_year(date) for date in dfirm['Founded Date']]

V = 1000 ; v = 2300
company_name = dfirm['Organization Name'][V:v].str.upper().tolist()
company_nat = dfirm['Jurisdiction'][V:v].str.upper().tolist()
company_date = dfirm['Founding Year'][V:v].fillna(2010).astype(int).tolist()
company_date = [datetime(year, 1, 1).strftime("%Y-%m-%d") for year in company_date]

dt = []
data = []
is_company_data = []
df = pd.DataFrame([])
t1 = tm.time()
lens_api_url = 'https://api.lens.org/patent/search'
headers = {'Authorization': 'Bearer tW5ZcjkE4q9ye8vvT8ByAfTa0LGFVoVSOZh4bDtg4DzKohRwES6N','Content-Type': 'application/json'}
size = 100

# Correction 2 : Utiliser `enumerate` pour avoir à la fois l'indice et la valeur de `company_name`
for j, name in enumerate(company_name):
    print(f'Patent extraction : {j} on {len(company_name)}')
    payload = {
        "query": {
            "bool": {
                "must": [
                    {
                        "match_phrase": {
                            "applicant.name": company_name[j]
                        }
                    },
                    {
                        "term": {
                            "jurisdiction": company_nat[j]
                        }
                    },
                    {
                        "range": {
                            "earliest_priority_claim_date": {
                                "gte": company_date[j],
                                "lte": "2023-12-31"
                            }
                        }
                    }
                ]
            }
        },
    "size": size,
    "scroll": "5m",
    }
    try:
        response = requests.post(lens_api_url, data=json.dumps(payload),headers=headers)

        if response.status_code == 200:  # Correction 7 : Vérifier si le status code est 200 (OK)
            res_json = response.json()
            data += res_json['data']
            is_company_data.append(1)
            _scroll_id = res_json.get('scroll_id')  # Correction 8 : Utiliser get() pour éviter KeyError si 'scroll_id' n'existe pas
            print(f'Tout est bon !')
            # Section de pagination
            # Correction 9 : Ce code devrait être dans le bloc 'if' précédent car il dépend de la réponse initiale
            if res_json['total'] > size:
                rows = []
                n = math.ceil(res_json['total'] / size)  # Correction 10 : Utiliser ceil pour s'assurer d'inclure toutes les pages
                for i in range(n):
                    print(f'Scrolling {i + 1} sur {n} :')
                    scroll_payload = json.dumps({
                        'scroll': '5m',
                        'scroll_id': _scroll_id
                    })
                    scroll_res = requests.post(
                        lens_api_url,
                        data=scroll_payload,
                        headers=headers
                    )
                    if scroll_res.status_code == 200 :
                        items = scroll_res.json()['data']
                        tm.sleep(2)  # Sleep for 4.5 seconds
                        rows += items
                    else : data += rows
                # Correction 13 : Vérification de doublons après la collecte de toutes les données
                if len(rows) + res_json['results'] != res_json['total']:
                    print(f'/!\ Valeurs manquantes')
                else:
                    print(f'Extraction complète')
                dt += data
            else:
                dt += data

        else:
            print('Pas de résultat, erreur', response.status_code)
            data = []
            dt += data
            is_company_data.append(0)
            res_json = []
            _scroll_id = None

    except Exception as e:
        print('Error:', e)
    df_t = pd.DataFrame(data)
    df_t['comp_name'] = company_name[j]
    df = pd.concat([df,df_t])
    data = []
    tm.sleep(4.2)

t2 = tm.time()
print(f'Temps total extraction : {(t2 - t1) / 60:.2f} min')
requete_restante = int(response.headers._store['x-rate-limit-remaining-request-per-month'][1]) - n
print(f'Nombre de requêtes restant ce mois-ci : {requete_restante} ')

df = df.reset_index(drop=True)

#df.to_csv(f'{path}/df2_B_GrenTech_{V}_{v}.csv')

df = pd.read_csv(f'{path}/df2_B_GrenTech_{V}_{v}.csv')
df_B = df.copy()

# Importation des différents df de données brevets extraites -----------------------------------------------------------

df1 = pd.read_csv(f'{path}/df2_B_GrenTech_100.csv')
df2 = pd.read_csv(f'{path}/df2_B_GrenTech_100_1000.csv')
df3 = pd.read_csv(f'{path}/df2_B_GrenTech_1000_2300.csv')

#Fusion des différentes bases extraites

df_B = pd.concat([df1, df2, df3]).sort_values(by='lens_id').reset_index(drop=True)
is_company_data = pd.DataFrame([is_company_data, company_name]).T

df_B.to_csv(f'{path}/df2_B_GrenTech_full.csv')

# Extraction des noms des acteurs : inv, app, et own ---
inv_name = []
for i in range(0, len(df_B)):
    print(f'Extract {i + 1} sur {len(df_B)}')
    try:
        for j in range(0, len(eval(df_B['biblio'][i])['parties']['inventors'])):
            try:
                inv_name.append([df_B['lens_id'][i], eval(df_B['biblio'][i])['parties']['inventors'][j]['extracted_name']['value']])
            except: j += 1 ; inv_name.append([df_B['lens_id'][i], np.nan])
    except:
        i += 1
inv_name = pd.DataFrame(inv_name, columns=['lens_id', 'inv_name'])

app_name = []
for i in range(0, len(df_B)):
    print(f'Extract {i + 1} sur {len(df_B)}')
    try:
        for j in range(0, len(eval(df_B['biblio'][i])['parties']['applicants'])):
            try:
                app_name.append([df_B['lens_id'][i], eval(df_B['biblio'][i])['parties']['applicants'][j]['extracted_name']['value']])
            except: j += 1
    except:
        i += 1
app_name = pd.DataFrame(app_name, columns=['lens_id','app_name'])

own_name = []
for i in range(0, len(df_B)):
    print(f'Extract {i + 1} sur {len(df_B)}')
    try:
        for j in range(0, len(eval(df_B['biblio'][i])['parties']['owners_all'])):
            try:
                own_name.append([df_B['lens_id'][i], eval(df_B['biblio'][i])['parties']['owners_all'][j]['extracted_name']['value']])
            except: j += 1
    except: own_name.append([df_B['lens_id'][i], np.nan]) ; i += 1
own_name = pd.DataFrame(own_name, columns=['lens_id','own_name'])

inv_app_own_names = pd.merge(pd.merge(inv_name, app_name, how='outer'), own_name, how="outer")
inv_app_own_names.to_csv(f'{path}/inv_app_own_names.csv')
# Vérification des valeurs ---------------------------------------------------------------------------------------------

x = pd.merge(df_b, inv_app_own_names, on='lens_id')

company_name = pd.DataFrame(company_name).sort_values(by=0).reset_index(drop=True)
